{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Annual Damage - Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from math import factorial, sqrt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import rv_discrete\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import Imputer, label_binarize, LabelEncoder, OneHotEncoder,\\\n",
    "MinMaxScaler, scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image  \n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats.distributions import poisson\n",
    "import pydot\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "random.seed(4444)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "subset_all = pd.read_pickle('./dfs/subset_all')\n",
    "\n",
    "## Model features\n",
    "\n",
    "features = ['INJURIES', 'DEATHS', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', \n",
    "            'EVENT_CATEGORY', 'BEGIN_YEAR', 'DURATION', 'STATE']\n",
    "df = subset_all[features]\n",
    "feat_cat = ['EVENT_CATEGORY', 'STATE']\n",
    "for item in feat_cat:\n",
    "    df[item] = df[item].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INJURIES 0.0\n",
      "DEATHS 0.0\n",
      "DAMAGE_PROPERTY 0.420281760126\n",
      "DAMAGE_CROPS 0.522487584474\n",
      "EVENT_CATEGORY 0.0\n",
      "BEGIN_YEAR 0.0\n",
      "DURATION 0.0\n",
      "STATE 0.0\n",
      "Percentage of data with damage property&crops info:  45.7573221454\n",
      "INJURIES              int64\n",
      "DEATHS                int64\n",
      "DAMAGE_PROPERTY     float64\n",
      "DAMAGE_CROPS        float64\n",
      "EVENT_CATEGORY     category\n",
      "BEGIN_YEAR            int64\n",
      "DURATION            float64\n",
      "STATE              category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "l = 1.0*len(df)\n",
    "for name in df.columns.values:\n",
    "    ln = len(df[df[name].isnull()])\n",
    "    print name, ln/l\n",
    "\n",
    "dfn = df.dropna()\n",
    "print 'Percentage of data with damage property&crops info: ', 100.0*len(dfn)/len(df)\n",
    "print dfn.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of train data:  72.4070419074\n"
     ]
    }
   ],
   "source": [
    "dfn_trainvalid = dfn.loc[dfn.BEGIN_YEAR < 2013, :]\n",
    "dfn_test = dfn.loc[dfn.BEGIN_YEAR >= 2013, :]\n",
    "print 'Percentage of train data: ', 100.0*len(dfn_trainvalid)/len(dfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Stats - Bootstrap - By Event Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_mean_stats(df, main_col):\n",
    "    return df.groupby([main_col]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_CI(col, size=1000):\n",
    "    n = len(col)\n",
    "    mean_list = []\n",
    "    for i in range(size):\n",
    "        sample = col.iloc[np.random.randint(0, n, size=round(n*0.3/0.7))]\n",
    "        mean_list.append(sample.mean())\n",
    "    mean_list = np.array(sorted(mean_list))\n",
    "    mean = np.mean(mean_list)\n",
    "    return (mean, np.percentile(mean_list,2.5), np.percentile(mean_list,97.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_means(df, main_col, damage_features):\n",
    "    predictions = defaultdict(dict)\n",
    "    for category in df[main_col].unique():\n",
    "        for feature in damage_features:\n",
    "            data = df.loc[df[main_col] == category, feature]\n",
    "            predictions[feature][category] = get_CI(data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bootstrap_accuracy(results, predictions):\n",
    "    count = 0\n",
    "    for category in results.index.values:\n",
    "        for feature in results.columns.values:\n",
    "            value = results.ix[category,feature]\n",
    "            val_tuple = predictions[feature][category]\n",
    "            if (val_tuple[1] <= float(value)) & (float(value) <= val_tuple[2]):\n",
    "                count += 1\n",
    "    dims = (results.shape[0])*(results.shape[1])\n",
    "    return 1.0*count/dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "damage_features = ['INJURIES','DEATHS','DAMAGE_PROPERTY','DAMAGE_CROPS']\n",
    "main_col = 'EVENT_CATEGORY'\n",
    "target_cols = [main_col] + damage_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predictions = bootstrap_means(dfn_trainvalid, main_col, damage_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = get_mean_stats(dfn_trainvalid, main_col)[damage_features]\n",
    "print results, '\\n'\n",
    "#print bootstrap_accuracy(results, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stormcounts = pd.read_csv(\"./dfs/avestormcounts.csv\")\n",
    "stormcounts = stormcounts.loc[stormcounts.EVENT_CATEGORY != 'Other',:]\n",
    "stormcounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ave_col(row, storm_dict):\n",
    "    return row[2]*storm_dict[row[1]][0]\n",
    "\n",
    "def CIlower_col(row, storm_dict):\n",
    "    return row[2]*storm_dict[row[1]][1]\n",
    "\n",
    "def CIupper_col(row, storm_dict):\n",
    "    return row[2]*storm_dict[row[1]][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for key in predictions:\n",
    "#    stormcounts[key] = stormcounts.apply(func = ave_col, axis = 1, storm_dict = predictions[key])\n",
    "#    stormcounts[key+\"_LOWER\"] = stormcounts.apply(func = CIlower_col, axis = 1, storm_dict = predictions[key])\n",
    "#    stormcounts[key+\"_UPPER\"] = stormcounts.apply(func = CIupper_col, axis = 1, storm_dict = predictions[key])\n",
    "#print stormcounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model2 -- Simply take the average total #INJURIES etc. per year per STATE per EVENT_CATEGORY; model independent of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test = dfn_trainvalid.groupby(['BEGIN_YEAR','STATE','EVENT_CATEGORY']).sum().reset_index()\n",
    "#sel = test[ (test.STATE == 'missouri') & (test.BEGIN_YEAR == 2011) & (test.EVENT_CATEGORY == 'Storm')]\n",
    "#sel\n",
    "#test2 = test.groupby(['STATE','EVENT_CATEGORY']).median().reset_index()\n",
    "#test2[ (test2.STATE == 'missouri')& (test2.EVENT_CATEGORY == 'Storm') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     STATE EVENT_CATEGORY  INJURIES  DEATHS  DAMAGE_PROPERTY  DAMAGE_CROPS  \\\n",
      "0  alabama          Flood       0.0     1.0         763800.0           0.0   \n",
      "1  alabama           Heat       0.5     0.0              0.0           0.0   \n",
      "\n",
      "   INJURIES_LOWER  DEATHS_LOWER  DAMAGE_PROPERTY_LOWER  DAMAGE_CROPS_LOWER  \\\n",
      "0             0.0           0.0                85164.0                 0.0   \n",
      "1             0.0           0.0                    0.0                 0.0   \n",
      "\n",
      "   INJURIES_UPPER  DEATHS_UPPER  DAMAGE_PROPERTY_UPPER  DAMAGE_CROPS_UPPER  \\\n",
      "0            3.20           5.2           2.746282e+08           5158840.0   \n",
      "1          165.65          10.1           3.929450e+04            237962.5   \n",
      "\n",
      "      AREA  DAMAGE_PROPERTY_PER_AREA  DAMAGE_CROPS_PER_AREA  \\\n",
      "0  52420.0                 14.570775                    0.0   \n",
      "1  52420.0                  0.000000                    0.0   \n",
      "\n",
      "   INJURIES_PER_AREA  DEATHS_PER_AREA  \n",
      "0           0.000000        19.076688  \n",
      "1           9.538344         0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Get the sum (on damage_features) per year, state, per event category\n",
    "dfdam = dfn_trainvalid.groupby(['BEGIN_YEAR','STATE','EVENT_CATEGORY']).sum()[damage_features]\n",
    "dfdam = dfdam.reset_index()\n",
    "# Calculate the 5th percentile, median and 95th percentile of these damages\n",
    "dfCIlower = dfdam.groupby(['STATE','EVENT_CATEGORY']).quantile(q=0.05)[damage_features]\n",
    "dfCIlower.rename(columns=lambda x: x+\"_LOWER\", inplace=True)\n",
    "dfCIlower = dfCIlower.reset_index()\n",
    "dfCIupper = dfdam.groupby(['STATE','EVENT_CATEGORY']).quantile(q=0.95)[damage_features]\n",
    "dfCIupper.rename(columns=lambda x: x+\"_UPPER\", inplace=True)\n",
    "dfCIupper = dfCIupper.reset_index()\n",
    "dfdam = dfdam.groupby(['STATE','EVENT_CATEGORY']).median()[damage_features]\n",
    "dfdam = dfdam.reset_index()\n",
    "merged = dfdam.merge(dfCIlower, on = ['STATE','EVENT_CATEGORY'])\n",
    "merged = merged.merge(dfCIupper, on = ['STATE','EVENT_CATEGORY'])\n",
    "\n",
    "areadf = pd.read_csv(\"./data/state_areas.csv\", \n",
    "                    dtype = {'AREA' : float, 'STATE' : str})\n",
    "areadf.STATE = areadf.STATE.str.lower()\n",
    "merged = pd.merge(merged,areadf,on='STATE')\n",
    "merged['DAMAGE_PROPERTY_PER_AREA'] = merged.DAMAGE_PROPERTY/merged.AREA\n",
    "merged['DAMAGE_CROPS_PER_AREA'] = merged.DAMAGE_CROPS/merged.AREA\n",
    "merged['INJURIES_PER_AREA'] = 1000000.0*merged.INJURIES/merged.AREA\n",
    "merged['DEATHS_PER_AREA'] = 1000000.0*merged.DEATHS/merged.AREA\n",
    "merged.to_csv('./data/damages_simple.csv', index=False)\n",
    "print merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabama 50.5387256772\n",
      "alaska 0.0\n",
      "american samoa 0.0\n",
      "arizona 0.622984472322\n",
      "arkansas 78.3952312003\n",
      "california 9.37502672653\n",
      "colorado 13.842584587\n",
      "connecticut 31.9109687895\n",
      "delaware 208.94837284\n",
      "district of columbia 0.0\n",
      "florida 74.3720535904\n",
      "georgia 3.22238115271\n",
      "hawaii 0.0\n",
      "idaho 0.0531297490696\n",
      "illinois 18.7163725524\n",
      "indiana 39.6820428336\n",
      "iowa 83.4030974002\n",
      "kansas 37.5566980238\n",
      "kentucky 3.83092456939\n",
      "louisiana 15.5312344878\n",
      "maine 22.5049462973\n",
      "maryland 16.1188134773\n",
      "massachusetts 187.464894827\n",
      "michigan 17.5571271998\n",
      "minnesota 10.0682168492\n",
      "mississippi 47.6090188305\n",
      "missouri 20.2480382171\n",
      "montana 4.42981501632\n",
      "nebraska 170.064642913\n",
      "nevada 2.12576420794\n",
      "new hampshire 22.0333725532\n",
      "new jersey 121.682907257\n",
      "new mexico 8.01028374044\n",
      "new york 7.29456511777\n",
      "north carolina 36.7741875546\n",
      "north dakota 25.1562632606\n",
      "ohio 276.769508767\n",
      "oklahoma 214.849318302\n",
      "oregon 1.48990130007\n",
      "pennsylvania 14.413080297\n",
      "puerto rico 3.45305164319\n",
      "rhode island 65.1877022654\n",
      "south carolina 30.3988132417\n",
      "south dakota 35.0033067068\n",
      "tennessee 17.5790503986\n",
      "texas 86.8348746817\n",
      "utah 2.81753183269\n",
      "vermont 252.418885191\n",
      "virginia 14.1347878434\n",
      "washington 13.5417543269\n",
      "west virginia 12.471110194\n",
      "wisconsin 57.9200332845\n",
      "wyoming 7.18258309223\n"
     ]
    }
   ],
   "source": [
    "# Test print for EVENT_CATEGORY == Storm, predicted DAMAGE_PROPERTY_PER_AREA based on the simple model\n",
    "dam_property_storm = merged.loc[merged.EVENT_CATEGORY == 'Storm',:]\n",
    "dam_property_storm = dam_property_storm[['STATE','DAMAGE_PROPERTY_PER_AREA']]\n",
    "for index, row in dam_property_storm.iterrows():\n",
    "    #row_string = '[\\'' + str(row['STATE']) +'\\',' + str(row['DAMAGE_PROPERTY_PER_AREA']) + '],'\n",
    "    #print row_string\n",
    "    print row['STATE'], row['DAMAGE_PROPERTY_PER_AREA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_simple_predictions(merged, damage_features):\n",
    "    predictions_simple = defaultdict(dict)\n",
    "    for index, row in merged.iterrows():\n",
    "        for feature in damage_features:\n",
    "            ci_lower = feature + '_LOWER'\n",
    "            ci_upper = feature + '_UPPER'\n",
    "            predictions_simple[feature][(row['STATE'],row['EVENT_CATEGORY'])]=\\\n",
    "                (row[ci_lower],row[ci_upper])\n",
    "    return predictions_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simple_accuracy(dftest, predictions, damage_features):\n",
    "    success = 0\n",
    "    for index, row in dftest.iterrows():\n",
    "        for feature in damage_features:\n",
    "            pred = predictions[feature][(row['STATE'],row['EVENT_CATEGORY'])]\n",
    "            if pred[0] <= row[feature] and row[feature] <= pred[1]:\n",
    "                success += 1\n",
    "    return 1.0*success/(len(damage_features)*len(dftest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for years 2013, 2014 and 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_13 = dfn_test.loc[dfn.BEGIN_YEAR == 2013,:]\n",
    "test_14 = dfn_test.loc[dfn.BEGIN_YEAR == 2014,:]\n",
    "test_15 = dfn_test.loc[dfn.BEGIN_YEAR == 2015,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test on 2013\n",
    "bystevent13 = test_13.groupby(['STATE','EVENT_CATEGORY']).sum()[damage_features]\n",
    "bystevent13 = bystevent13.reset_index()\n",
    "bystevent13 = bystevent13[ bystevent13.STATE != 'virgin islands']\n",
    "# Test on 2014\n",
    "bystevent14 = test_14.groupby(['STATE','EVENT_CATEGORY']).sum()[damage_features]\n",
    "bystevent14 = bystevent14.reset_index()\n",
    "bystevent14 = bystevent14[ bystevent14.STATE != 'virgin islands']\n",
    "# Test on 2015\n",
    "bystevent15 = test_15.groupby(['STATE','EVENT_CATEGORY']).sum()[damage_features]\n",
    "bystevent15 = bystevent15.reset_index()\n",
    "bystevent15 = bystevent15[ bystevent15.STATE != 'virgin islands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.633254716981\n",
      "0.633844339623\n",
      "0.624410377358\n"
     ]
    }
   ],
   "source": [
    "predictions = get_simple_predictions(merged, damage_features)\n",
    "print get_simple_accuracy(bystevent13, predictions, damage_features)\n",
    "print get_simple_accuracy(bystevent14, predictions, damage_features)\n",
    "print get_simple_accuracy(bystevent15, predictions, damage_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
