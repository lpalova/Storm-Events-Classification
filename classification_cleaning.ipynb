{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import factorial, sqrt, fabs, floor, isnan\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data and Exploring Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latlonabrev = pd.read_csv(\"./data/state_latlon.csv\")\n",
    "abrevDF = pd.read_csv(\"./data/state_abrev.csv\")\n",
    "latlonDF = pd.merge(latlonabrev,abrevDF,on=\"ABBREVIATION\")\n",
    "latlonDF = latlonDF.drop('ABBREVIATION',axis = 1)\n",
    "#print latlonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/StormEvents_details-ftp_v1.0_d1996_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d1997_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d1998_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d1999_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2000_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2001_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2002_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2003_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2004_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2005_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2006_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2007_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2008_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2009_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2010_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2011_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2012_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2013_c20160223.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2014_c20160419.csv\n",
      "./data/StormEvents_details-ftp_v1.0_d2015_c20160419.csv\n"
     ]
    }
   ],
   "source": [
    "details = pd.DataFrame()\n",
    "pathdate1 = \"_c20160223.csv\"\n",
    "pathdate2 = \"_c20160419.csv\"\n",
    "for year in range(1996, 2016):\n",
    "    path = \"./data/StormEvents_details-ftp_v1.0_d\" + str(year)\n",
    "    #path = \"ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/StormEvents_details-ftp_v1.0_d\" + str(year)\n",
    "    if year <= 2013:\n",
    "        path += \"_c20160223.csv\"\n",
    "    else:\n",
    "        path += \"_c20160419.csv\"\n",
    "    print path\n",
    "    df = pd.read_csv(path,\n",
    "                   dtype = {'BEGIN_TIME' : str,\n",
    "                            'BEGIN_YEARMONTH' : str,\n",
    "                            'BEGIN_DATE_TIME' : str,\n",
    "                            'END_TIME' : str,\n",
    "                            'END_YEARMONTH' : str,\n",
    "                            'END_DATE_TIME' : str,\n",
    "                            'BEGIN_AZIMUTH' : str,\n",
    "                            'END_AZIMUTH' : str,\n",
    "                            'FLOOD_CAUSE' : str,\n",
    "                            'TOR_OTHER_WFO' : str,\n",
    "                            'TOR_OTHER_CZ_STATE' : str,\n",
    "                            'TOR_OTHER_CZ_NAME' : str,\n",
    "                            'DAMAGE_CROPS' : str,\n",
    "                            'DAMAGE_PROPERTY' : str,\n",
    "                            'SOURCE' : str,\n",
    "                            'MAGNITUDE' : float,\n",
    "                            'MAGNITUDE_TYPE': str\n",
    "                           })\n",
    "    details = details.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TIMES\n",
    "details['BEGIN_DT'] = pd.to_datetime(details.BEGIN_DATE_TIME, \n",
    "                                     format=\"%d-%b-%y %H:%M:%S\", errors = 'coerce')\n",
    "details['END_DT'] = pd.to_datetime(details.END_DATE_TIME, \n",
    "                                   format=\"%d-%b-%y %H:%M:%S\", errors = 'coerce')\n",
    "details['BEGIN_DAY'] = details['BEGIN_DT'].map(lambda x: x.day)\n",
    "# 0 -- Monday, 6 -- Sunday\n",
    "details['BEGIN_WEEKDAY'] = details['BEGIN_DT'].map(lambda x: x.weekday())\n",
    "details['BEGIN_MONTH'] = details['BEGIN_DT'].map(lambda x: x.month)\n",
    "details['BEGIN_YEAR'] = details['BEGIN_DT'].map(lambda x: x.year)\n",
    "details['BEGIN_HOUR'] = details['BEGIN_DT'].map(lambda x: x.hour)\n",
    "details['END_DAY'] = details['END_DT'].map(lambda x: x.day)\n",
    "details['END_WEEKDAY'] = details['END_DT'].map(lambda x: x.weekday())\n",
    "details['END_MONTH'] = details['END_DT'].map(lambda x: x.month)\n",
    "details['END_YEAR'] = details['END_DT'].map(lambda x: x.year)\n",
    "details['END_HOUR'] = details['END_DT'].map(lambda x: x.hour)\n",
    "details['DURING_WEEKEND'] = (details['BEGIN_WEEKDAY'] >= 5) | (details['END_WEEKDAY'] >= 5)\n",
    "details['BEGIN_MORNING'] = (details['BEGIN_HOUR'] <= 8) \n",
    "details['BEGIN_DAY'] = ((details['BEGIN_HOUR'] > 8) & (details['BEGIN_HOUR'] <= 16)) \n",
    "details['BEGIN_EVENING'] = (details['BEGIN_HOUR'] > 16) \n",
    "\n",
    "details['DURATION'] = details['END_DT'] - details['BEGIN_DT']\n",
    "details['DURATION_SECONDS'] = details['DURATION'].map(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ignore observations with BEGIN timestamp > END timestamp\n",
    "# Ignore observations with durations > 3 months\n",
    "# 43 observations \n",
    "details = details.loc[details.BEGIN_DT <= details.END_DT, :]\n",
    "details = details.loc[details.DURATION < datetime.timedelta(seconds = 7776000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seasons_dict = {\n",
    "    3: 'spring', 4: 'spring', 5: 'spring',\n",
    "    6: 'summer', 7: 'summer', 8: 'summer',\n",
    "    9: 'fall', 10: 'fall', 11: 'fall',\n",
    "    12: 'winter', 1: 'winter', 2: 'winter'\n",
    "}\n",
    "details['SEASON'] = details.BEGIN_MONTH.map(lambda x: seasons_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DURATION measured in units of days\n",
    "details['DURATION'] = details.DURATION_SECONDS/86400.0\n",
    "details['DURATION_HOURS'] = details.DURATION_SECONDS.map(lambda x: int(floor(x/3600.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def money_to_int(s):\n",
    "    money_dict = { 'K' : 1e3, 'k' : 1e3, 'M' : 1e6, 'm' : 1e6, \n",
    "                   'G' : 1e9, 'g' : 1e9, 'T' : 1e12, 't' : 1e12,\n",
    "                   'B' : 1e9, 'b' : 1e9}\n",
    "    try:\n",
    "        if len(s) == 1:\n",
    "            dec = money_dict[s]\n",
    "        else:\n",
    "            dec = money_dict[re.split('[0-9.]+',s)[1]]\n",
    "    except:\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            formats.append(s)\n",
    "            return None\n",
    "    try:\n",
    "        num = float(re.split('[a-z]+', s, flags=re.IGNORECASE)[0])\n",
    "        return num*dec\n",
    "    except:\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "details['DAMAGE_CROPS'] = details['DAMAGE_CROPS'].map(lambda x: money_to_int(x))\n",
    "details['DAMAGE_PROPERTY'] = details['DAMAGE_PROPERTY'].map(lambda x: money_to_int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.520487669754\n",
      "0.420655115347\n"
     ]
    }
   ],
   "source": [
    "# About 52% of the observations has missing DAMAGE_CROPS values.\n",
    "# About 42% of the observations has mising DAMAGE_PROPERTY values.\n",
    "print 1.0*sum(details.DAMAGE_CROPS.isnull())/len(details)\n",
    "print 1.0*sum(details.DAMAGE_PROPERTY.isnull())/len(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adjusting for inflation: 1996 - 2016\n",
    "# http://data.bls.gov/cgi-bin/cpicalc.pl\n",
    "inflation_dict = {\n",
    "                  1996 : 1.52,\n",
    "                  1997 : 1.48,\n",
    "                  1998 : 1.46,\n",
    "                  1999 : 1.43,\n",
    "                  2000 : 1.38,\n",
    "                  2001 : 1.34,\n",
    "                  2002 : 1.32,\n",
    "                  2003 : 1.29,\n",
    "                  2004 : 1.26,\n",
    "                  2005 : 1.22,\n",
    "                  2006 : 1.18,\n",
    "                  2007 : 1.15,\n",
    "                  2008 : 1.11,\n",
    "                  2009 : 1.11,\n",
    "                  2010 : 1.09,\n",
    "                  2011 : 1.06,\n",
    "                  2012 : 1.04,\n",
    "                  2013 : 1.02,\n",
    "                  2014 : 1.01,\n",
    "                  2015 : 1.00,\n",
    "                  2016 : 1.00\n",
    "                 }\n",
    "details['INFLATION'] = details['BEGIN_YEAR'].map(lambda x: inflation_dict[x])\n",
    "details['DAMAGE_CROPS'] = details['DAMAGE_CROPS']*details['INFLATION']\n",
    "details['DAMAGE_PROPERTY'] = details['DAMAGE_PROPERTY']*details['INFLATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting LAT, LON according to latlonDF for missing LAT, LON values\n",
    "latlonDF.STATE = latlonDF.STATE.str.lower()\n",
    "details.STATE = details.STATE.str.lower()\n",
    "details = pd.merge(details,latlonDF,on='STATE')\n",
    "details.loc[details.BEGIN_LAT.isnull(),'BEGIN_LAT'] = \\\n",
    "    details.loc[details.BEGIN_LAT.isnull(),'LATITUDE']\n",
    "details.loc[details.BEGIN_LON.isnull(),'BEGIN_LON'] = \\\n",
    "    details.loc[details.BEGIN_LON.isnull(),'LONGITUDE']\n",
    "details = details.drop(['LATITUDE','LONGITUDE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LAT, LON measured in units of 1/100 of LAT, LON\n",
    "details['LAT'] = details.BEGIN_LAT/100.0\n",
    "details['LON'] = details.BEGIN_LON/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN_HOUR             int64\n",
      "BEGIN_YEAR             int64\n",
      "SEASON                object\n",
      "DURATION_HOURS         int64\n",
      "DURATION             float64\n",
      "DURING_WEEKEND          bool\n",
      "BEGIN_MORNING           bool\n",
      "BEGIN_DAY               bool\n",
      "BEGIN_EVENING           bool\n",
      "STATE                 object\n",
      "EVENT_TYPE            object\n",
      "WFO                   object\n",
      "INJURIES_DIRECT        int64\n",
      "INJURIES_INDIRECT      int64\n",
      "DEATHS_DIRECT          int64\n",
      "DEATHS_INDIRECT        int64\n",
      "DAMAGE_PROPERTY      float64\n",
      "DAMAGE_CROPS         float64\n",
      "MAGNITUDE            float64\n",
      "CATEGORY             float64\n",
      "TOR_F_SCALE           object\n",
      "BEGIN_RANGE          float64\n",
      "BEGIN_AZIMUTH         object\n",
      "END_RANGE            float64\n",
      "END_AZIMUTH           object\n",
      "LAT                  float64\n",
      "LON                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "subset_all = details[['BEGIN_HOUR',\n",
    "                    'BEGIN_YEAR',\n",
    "                    'SEASON',\n",
    "                    'DURATION_HOURS',\n",
    "                    'DURATION',\n",
    "                    'DURING_WEEKEND',\n",
    "                    'BEGIN_MORNING',\n",
    "                    'BEGIN_DAY',\n",
    "                    'BEGIN_EVENING',\n",
    "                    'STATE',\n",
    "                    'EVENT_TYPE',\n",
    "                    'WFO',\n",
    "                    'INJURIES_DIRECT',\n",
    "                    'INJURIES_INDIRECT',\n",
    "                    'DEATHS_DIRECT',           \n",
    "                    'DEATHS_INDIRECT',         \n",
    "                    'DAMAGE_PROPERTY',        \n",
    "                    'DAMAGE_CROPS',\n",
    "                    'MAGNITUDE',\n",
    "                    'CATEGORY',\n",
    "                    'TOR_F_SCALE',\n",
    "                    'BEGIN_RANGE',\n",
    "                    'BEGIN_AZIMUTH',\n",
    "                    'END_RANGE',\n",
    "                    'END_AZIMUTH',\n",
    "                    'LAT',\n",
    "                    'LON'\n",
    "                   ]]\n",
    "print subset_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storm Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def storm_def(s):\n",
    "    if re.search('[Ww]ind|[Dd]ust', s):\n",
    "        return 'Wind'\n",
    "    if re.search('[Ss]torm|Tropical Depression|[Ll]ightning|[Hh]ail|[Rr]ain|[Ff]og|[Bb]lizzard', s):\n",
    "        return 'Storm'\n",
    "    if re.search('Northern Lights', s):\n",
    "        return 'Storm'\n",
    "    if re.search('[Tt]ornado|Funnel Cloud|[Ww]aterspout', s):\n",
    "        return 'Tornado'\n",
    "    if re.search('[Hh]urricane', s):\n",
    "        return 'Hurricane'\n",
    "    if re.search('[Ff]lood|Rip Current|Debris Flow|[Ll]andslide', s):\n",
    "        return 'Flood'\n",
    "    if re.search('[Hh]eat|[Aa]sh|[Dd]rought|[Ff]ire|[Ss]moke', s):\n",
    "        return 'Heat'\n",
    "    if re.search('[Ss]now|[Aa]valanche|[Ww]inter|[Ff]rost|[Ff]reez|[Ss]leet', s):\n",
    "        return 'Winter Weather'\n",
    "    if re.search('[Tt]ide|[Tt]sunami|[Ss]urf|[Ww]ave|[Ss]eiche', s):\n",
    "        return 'Tide'\n",
    "    print s\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHER\n",
      "EVENT_CATEGORY\n",
      "Storm             384624\n",
      "Wind              366671\n",
      "Flood             115248\n",
      "Winter Weather    112933\n",
      "Heat               77756\n",
      "Tornado            36174\n",
      "Tide                7718\n",
      "Hurricane           1729\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucia/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "subset_all['EVENT_CATEGORY'] = subset_all['EVENT_TYPE'].map(lambda x: storm_def(x))\n",
    "print subset_all.groupby('EVENT_CATEGORY').size().sort_values(ascending=False)\n",
    "subset_all = subset_all.loc[ subset_all.EVENT_CATEGORY.notnull(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# No good way to substitute for nans in MAGNITUDE -- too many data points with nan\n",
    "# No good way to substitute for nans in CATEGORY  -- too many data points with nan\n",
    "\n",
    "# Setting TOR_F_SCALE nan values to 'N/A'\n",
    "subset_all.loc[subset_all.TOR_F_SCALE.isnull(),'TOR_F_SCALE'] = 'N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting impact of storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_yes(x):\n",
    "    if isnan(x):\n",
    "        return None\n",
    "    if x == 0.0:\n",
    "        return int(0)\n",
    "    else:\n",
    "        return int(1)\n",
    "    \n",
    "subset_all['INJURIES'] = subset_all['INJURIES_DIRECT'] + subset_all['INJURIES_INDIRECT']\n",
    "subset_all['DEATHS'] = subset_all['DEATHS_DIRECT'] + subset_all['DEATHS_INDIRECT']\n",
    "subset_all['INJURIES_YES'] = subset_all.INJURIES.map(lambda x: int(x != 0))\n",
    "subset_all['DEATHS_YES'] = subset_all.DEATHS.map(lambda x: int(x != 0))\n",
    "subset_all['DAMAGE_PROPERTY_YES'] = subset_all.DAMAGE_PROPERTY.map(lambda x: get_yes(x))\n",
    "subset_all['DAMAGE_CROPS_YES'] = subset_all.DAMAGE_CROPS.map(lambda x: get_yes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_all.to_pickle('./dfs/subset_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_trainvalid, subset_test = train_test_split(subset_all, test_size = 0.2,\n",
    "                                                  random_state=123)\n",
    "subset_trainvalid.to_pickle('./dfs/subset_trainvalid')\n",
    "subset_test.to_pickle('./dfs/subset_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
